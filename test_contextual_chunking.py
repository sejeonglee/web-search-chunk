"""Contextual Retrieval ì²­í‚¹ ë‹¨ìœ„ í…ŒìŠ¤íŠ¸"""

import asyncio
import os
from dotenv import load_dotenv
from src.adapters.llm_adapter import VLLMAdapter
from src.adapters.chunking_adapter import ContextualChunkingAdapter
from src.core.models import WebDocumentContent
from src.utils.logger import setup_logger, set_global_log_level

# ì „ì²´ ë¡œê·¸ ë ˆë²¨ì„ DEBUGë¡œ ì„¤ì •
set_global_log_level("DEBUG")

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()

async def test_contextual_chunking():
    """Contextual Chunking ë‹¨ìœ„ í…ŒìŠ¤íŠ¸"""
    print("ğŸ§ª Contextual Chunking í…ŒìŠ¤íŠ¸ ì‹œì‘")
    print("=" * 60)
    
    # LLM ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
    llm_service = VLLMAdapter(
        model_name="qwen3:4b",
        embedding_model="bge-large:335m",
        base_url="http://localhost:11434/v1"
    )
    
    # Contextual Chunking Adapter ì´ˆê¸°í™”
    chunking_service = ContextualChunkingAdapter(
        llm_service=llm_service,
        chunk_size=500,  # í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ ë” ì‘ì€ ì²­í¬ ì‚¬ì´ì¦ˆ ì‚¬ìš©
        overlap=100
    )
    
    # í…ŒìŠ¤íŠ¸ìš© ë¬¸ì„œ ìƒì„±
    test_document = WebDocumentContent(
        url="https://test.com/ai-chip-news",
        content="""
AI ë°˜ë„ì²´ ì‹œì¥ ë™í–¥

ìµœê·¼ ì¸ê³µì§€ëŠ¥ ë°˜ë„ì²´ ì‹œì¥ì—ì„œëŠ” NVIDIA ì™¸ì—ë„ ë‹¤ì–‘í•œ ê¸°ì—…ë“¤ì´ ë‘ê°ì„ ë‚˜íƒ€ë‚´ê³  ìˆë‹¤.

AMDì˜ ê²½ìš°, Instinct MI ì‹œë¦¬ì¦ˆë¥¼ í†µí•´ ë°ì´í„°ì„¼í„°ìš© AI ê°€ì†ê¸° ì‹œì¥ì— ì§„ì¶œí•˜ê³  ìˆìœ¼ë©°, 2024ë…„ ìƒë°˜ê¸°ì—ëŠ” MI300X ì¹©ì„ ì¶œì‹œí•˜ì—¬ ChatGPTì™€ ê°™ì€ ëŒ€í˜• ì–¸ì–´ëª¨ë¸ í›ˆë ¨ì— ì‚¬ìš©ë˜ê³  ìˆë‹¤.

Intelì€ Gaudi ì‹œë¦¬ì¦ˆë¥¼ í†µí•´ AI í›ˆë ¨ ë° ì¶”ë¡  ì‹œì¥ì— ë„ì „ì¥ì„ ë‚´ë°€ì—ˆë‹¤. Gaudi3ì€ íŠ¹íˆ ëŒ€í™”í˜• AI ì• í”Œë¦¬ì¼€ì´ì…˜ì— ìµœì í™”ë˜ì–´ ìˆìœ¼ë©°, OpenAI ë“±ì˜ ê¸°ì—…ë“¤ê³¼ íŒŒíŠ¸ë„ˆì‹­ì„ ë§ºê³  ìˆë‹¤.

Qualcommì€ ëª¨ë°”ì¼ AI ì¹© ì‹œì¥ì—ì„œ ë…ë³´ì ì¸ ìœ„ì¹˜ë¥¼ ì°¨ì§€í•˜ê³  ìˆë‹¤. Snapdragon 8 Gen 3ì€ ì˜¨ë””ë°”ì´ìŠ¤ AI ì²˜ë¦¬ì— íŠ¹í™”ë˜ì–´ ìˆìœ¼ë©°, ìŠ¤ë§ˆíŠ¸í°ì—ì„œì˜ ì‹¤ì‹œê°„ AI ê¸°ëŠ¥ì„ ê°€ëŠ¥í•˜ê²Œ í•œë‹¤.

Googleì˜ TPU (Tensor Processing Unit)ëŠ” ìì‚¬ í´ë¼ìš°ë“œ ì„œë¹„ìŠ¤ì™€ AI ì—°êµ¬ì— í•µì‹¬ì ì¸ ì—­í• ì„ í•˜ê³  ìˆìœ¼ë©°, íŠ¹íˆ Transformer ëª¨ë¸ í•™ìŠµì— ìµœì í™”ë˜ì–´ ìˆë‹¤.

ì´ëŸ¬í•œ ë‹¤ì–‘í•œ ê¸°ì—…ë“¤ì˜ ê²½ìŸìœ¼ë¡œ ì¸í•´ AI ë°˜ë„ì²´ ì‹œì¥ì€ ë”ìš± í˜ì‹ ì ì´ê³  ê²½ìŸì ìœ¼ë¡œ ë°œì „í•˜ê³  ìˆë‹¤.
        """.strip(),
        crawl_datetime="2024-12-15T10:00:00"
    )
    
    print(f"ğŸ“„ í…ŒìŠ¤íŠ¸ ë¬¸ì„œ ê¸¸ì´: {len(test_document.content)}ì")
    print("-" * 60)
    
    # í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬
    test_query = "AI ë°˜ë„ì²´ ê¸°ì—…ë“¤ì˜ ìµœì‹  ë™í–¥"
    
    try:
        # Contextual Chunking ì‹¤í–‰
        chunks = await chunking_service.chunk_document(test_document, test_query)
        
        print(f"âœ… ì²­í‚¹ ì™„ë£Œ: {len(chunks)}ê°œ ì²­í¬ ìƒì„±")
        print("=" * 60)
        
        # ê° ì²­í¬ì˜ ì»¨í…ìŠ¤íŠ¸ í™•ì¸
        for i, chunk in enumerate(chunks):
            print(f"ğŸ“ ì²­í¬ {i+1}:")
            print(f"   ID: {chunk.chunk_id}")
            print(f"   ì›ë³¸ ê¸¸ì´: {len(chunk.metadata.get('original_content', ''))}")
            print(f"   ì»¨í…ìŠ¤íŠ¸ í¬í•¨ ê¸¸ì´: {len(chunk.content)}")
            print(f"   Contextual Retrieval ì ìš©: {chunk.metadata.get('contextual_retrieval', False)}")
            
            # ì»¨í…ìŠ¤íŠ¸ê°€ ì¶”ê°€ëœ ê²½ìš°ì™€ ì›ë³¸ ë¹„êµ
            if chunk.metadata.get('contextual_retrieval'):
                original = chunk.metadata.get('original_content', '')
                if len(chunk.content) > len(original):
                    context_part = chunk.content[:len(chunk.content) - len(original)]
                    print(f"   ğŸ” ì¶”ê°€ëœ ì»¨í…ìŠ¤íŠ¸: {context_part[:100]}...")
            
            print(f"   ğŸ“„ ë‚´ìš© (ì²˜ìŒ 150ì): {chunk.content[:150]}...")
            print("-" * 40)
            
    except Exception as e:
        print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {str(e)}")
        import traceback
        traceback.print_exc()
    
    print("ğŸ Contextual Chunking í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
    
    # ë¦¬ì†ŒìŠ¤ ì •ë¦¬
    await llm_service.__aexit__(None, None, None)

if __name__ == "__main__":
    asyncio.run(test_contextual_chunking())