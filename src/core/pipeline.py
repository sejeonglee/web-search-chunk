import asyncio
from typing import TypedDict, List, Annotated, Sequence
from datetime import datetime
from langgraph.graph import StateGraph, END

from .models import (
    SearchQuery,
    WebDocument,
    WebDocumentContent,
    SemanticChunk,
    ScratchPad,
    QAResponse,
    ILLMService,
    IWebSearchService,
    ICrawlingService,
    IVectorStore,
    IChunkingService,
    IRetrievalService,
    IRerankingService,
)
from ..utils.logger import setup_logger

logger = setup_logger(__name__)


class PipelineState(TypedDict):
    """íŒŒì´í”„ë¼ì¸ ìƒíƒœ ì •ì˜."""

    user_query: str
    search_queries: List[SearchQuery]
    web_documents: List[WebDocument]
    document_contents: List[WebDocumentContent]
    chunks: List[SemanticChunk]
    scratch_pad: ScratchPad
    response: QAResponse
    start_time: float


class QAPipeline:
    """ì›¹ ê²€ìƒ‰ ê¸°ë°˜ QA íŒŒì´í”„ë¼ì¸ - ì¸í„°í˜ì´ìŠ¤ì—ë§Œ ì˜ì¡´."""

    def __init__(
        self,
        llm_service: ILLMService,
        search_service: IWebSearchService,
        crawling_service: ICrawlingService,
        vector_store: IVectorStore,
        chunking_service: IChunkingService,
        retrieval_service: IRetrievalService,
        reranking_service: IRerankingService,
    ):
        """ëª¨ë“  ì˜ì¡´ì„±ì€ ì¸í„°í˜ì´ìŠ¤ë¡œ ì£¼ì…ë°›ìŒ."""
        self.llm_service = llm_service
        self.search_service = search_service
        self.crawling_service = crawling_service
        self.vector_store = vector_store
        self.chunking_service = chunking_service
        self.retrieval_service = retrieval_service
        self.reranking_service = reranking_service
        self.graph = self._build_graph()

    def _build_graph(self) -> StateGraph:
        """LangGraph ì›Œí¬í”Œë¡œìš° êµ¬ì„±."""
        workflow = StateGraph(PipelineState)

        # ë…¸ë“œ ì¶”ê°€
        workflow.add_node("generate_queries", self.generate_search_queries)
        workflow.add_node("search_web", self.search_web)
        workflow.add_node("crawl_documents", self.crawl_documents)
        workflow.add_node("chunk_documents", self.chunk_documents)
        workflow.add_node("store_vectors", self.store_vectors)
        workflow.add_node("retrieve_chunks", self.retrieve_chunks)
        workflow.add_node("generate_answer", self.generate_answer)

        # ì—£ì§€ ì •ì˜
        workflow.set_entry_point("generate_queries")
        workflow.add_edge("generate_queries", "search_web")
        workflow.add_edge("search_web", "crawl_documents")
        workflow.add_edge("crawl_documents", "chunk_documents")
        workflow.add_edge("chunk_documents", "store_vectors")
        workflow.add_edge("store_vectors", "retrieve_chunks")
        workflow.add_edge("retrieve_chunks", "generate_answer")
        workflow.add_edge("generate_answer", END)

        return workflow.compile()

    async def generate_search_queries(self, state: PipelineState) -> PipelineState:
        """ê²€ìƒ‰ ì§ˆì˜ ìƒì„± (ë©€í‹° ì¿¼ë¦¬ ì¬ì‘ì„±)."""
        queries = await self.llm_service.generate_queries(state["user_query"])
        state["search_queries"] = queries
        return state

    async def search_web(self, state: PipelineState) -> PipelineState:
        """ì›¹ ê²€ìƒ‰ (ë³‘ë ¬ ì²˜ë¦¬)."""
        logger.info(f"ğŸ” ê²€ìƒ‰ ì¿¼ë¦¬ ê°œìˆ˜: {len(state['search_queries'])}")
        tasks = [
            self.search_service.search(query.processed_queries[0], max_results=7)
            for query in state["search_queries"]
        ]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        all_documents = []
        for i, result in enumerate(results):
            if isinstance(result, list):
                logger.info(f"  ì¿¼ë¦¬ {i+1}: {len(result)}ê°œ ë¬¸ì„œ ë°œê²¬")
                all_documents.extend(result)
            elif isinstance(result, Exception):
                logger.error(f"  ì¿¼ë¦¬ {i+1} ê²€ìƒ‰ ì‹¤íŒ¨: {str(result)}")
        
        logger.info(f"ğŸ“‘ ì´ {len(all_documents)}ê°œ ì›¹ ë¬¸ì„œ ë°œê²¬")
        state["web_documents"] = all_documents
        return state

    async def crawl_documents(self, state: PipelineState) -> PipelineState:
        """ë¬¸ì„œ í¬ë¡¤ë§ (ë³‘ë ¬ ì²˜ë¦¬)."""
        unique_urls = list({doc.url for doc in state["web_documents"]})
        logger.info(f"ğŸ•·ï¸  í¬ë¡¤ë§í•  URL ê°œìˆ˜: {len(unique_urls)}")
        
        if not unique_urls:
            logger.warning("âš ï¸  í¬ë¡¤ë§í•  URLì´ ì—†ìŠµë‹ˆë‹¤.")
            state["document_contents"] = []
            return state
            
        tasks = [self.crawling_service.crawl(url) for url in unique_urls[:10]]
        contents = await asyncio.gather(*tasks, return_exceptions=True)
        
        successful_contents = []
        for i, content in enumerate(contents):
            if isinstance(content, WebDocumentContent):
                logger.debug(f"  âœ… URL {i+1} í¬ë¡¤ë§ ì„±ê³µ: {unique_urls[i]}")
                successful_contents.append(content)
            else:
                logger.error(f"  âŒ URL {i+1} í¬ë¡¤ë§ ì‹¤íŒ¨: {unique_urls[i]} - {str(content)}")
        
        state["document_contents"] = successful_contents
        logger.info(f"ğŸ“„ ì„±ê³µì ìœ¼ë¡œ í¬ë¡¤ë§ëœ ë¬¸ì„œ: {len(successful_contents)}ê°œ")
        return state

    async def chunk_documents(self, state: PipelineState) -> PipelineState:
        """ë¬¸ì„œ ì²­í‚¹ (ì œí•œëœ ë³‘ë ¬ ì²˜ë¦¬ - í™˜ê²½ë³€ìˆ˜ë¡œ ë™ì‹œ ì‹¤í–‰ ìˆ˜ ì¡°ì ˆ)."""
        document_contents = state["document_contents"]
        logger.info(f"ğŸ“„ í¬ë¡¤ë§ëœ ë¬¸ì„œ ê°œìˆ˜: {len(document_contents)}")
        
        if not document_contents:
            logger.warning("âš ï¸  ì²­í‚¹í•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
            state["chunks"] = []
            return state
        
        # í™˜ê²½ë³€ìˆ˜ì—ì„œ ë™ì‹œ ì‹¤í–‰ ìˆ˜ ê°€ì ¸ì˜¤ê¸° (ê¸°ë³¸ê°’: 2)
        import os
        max_concurrent_chunks = int(os.getenv("MAX_CONCURRENT_CHUNKS", "2"))
        logger.info(f"ğŸ”§ ìµœëŒ€ ë™ì‹œ ì²­í‚¹ ë¬¸ì„œ ìˆ˜: {max_concurrent_chunks}ê°œ")
        
        # Semaphoreë¡œ ë™ì‹œ ì‹¤í–‰ ìˆ˜ ì œí•œ
        semaphore = asyncio.Semaphore(max_concurrent_chunks)
        
        async def limited_chunk_document(content):
            async with semaphore:
                logger.debug(f"ğŸ”§ ë¬¸ì„œ ì²­í‚¹ ì‹œì‘: {content.url}")
                return await self.chunking_service.chunk_document(content, state["user_query"])
        
        # ì œí•œëœ ë³‘ë ¬ë¡œ ì²­í‚¹ ì‘ì—… ìˆ˜í–‰
        tasks = [limited_chunk_document(content) for content in document_contents]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        all_chunks = []
        for i, result in enumerate(results):
            if isinstance(result, list):
                logger.debug(f"  âœ… ë¬¸ì„œ {i+1} ì²­í‚¹ ì„±ê³µ: {len(result)}ê°œ chunk ìƒì„±")
                all_chunks.extend(result)
            elif isinstance(result, Exception):
                logger.error(f"  âŒ ë¬¸ì„œ {i+1} ì²­í‚¹ ì‹¤íŒ¨: {str(result)}")
        
        state["chunks"] = all_chunks
        logger.info(f"ğŸ“Š ì´ {len(all_chunks)}ê°œ chunk ìƒì„± ì™„ë£Œ")
        return state

    async def store_vectors(self, state: PipelineState) -> PipelineState:
        """ë²¡í„° ì €ì¥ (ì„ë² ë”© ìƒì„± í¬í•¨)."""
        chunks = state["chunks"]
        logger.info(f"ğŸ”¢ ìƒì„±ëœ chunks ê°œìˆ˜: {len(chunks)}")
        
        if not chunks:
            logger.warning("âš ï¸  ì €ì¥í•  chunksê°€ ì—†ìŠµë‹ˆë‹¤.")
            return state
            
        # ì„ë² ë”©ì´ ì—†ëŠ” chunksì— ì„ë² ë”© ìƒì„±
        chunks_without_embedding = [chunk for chunk in chunks if not chunk.embedding]
        if chunks_without_embedding:
            logger.info(f"ğŸ”® {len(chunks_without_embedding)}ê°œ chunkì— ì„ë² ë”© ìƒì„± ì¤‘...")
            texts = [chunk.content for chunk in chunks_without_embedding]
            
            try:
                embeddings = await self.llm_service.get_embeddings(texts)
                for chunk, embedding in zip(chunks_without_embedding, embeddings):
                    chunk.embedding = embedding
                logger.info("âœ… ì„ë² ë”© ìƒì„± ì™„ë£Œ")
            except Exception as e:
                logger.error(f"âŒ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {str(e)}")
                # ì„ë² ë”© ì—†ì´ë„ ì €ì¥í•  ìˆ˜ ìˆë„ë¡ ì„ì‹œ ì„ë² ë”© ìƒì„±
                for chunk in chunks_without_embedding:
                    chunk.embedding = [0.0] * 1024  # ì„ì‹œ ë”ë¯¸ ì„ë² ë”© (bge-large:335m ì°¨ì›)
                logger.warning("âš ï¸  ë”ë¯¸ ì„ë² ë”©ìœ¼ë¡œ ëŒ€ì²´í–ˆìŠµë‹ˆë‹¤.")
        
        if chunks:
            logger.debug(f"ğŸ“ ì²« ë²ˆì§¸ chunk ID: {chunks[0].chunk_id}")
            logger.debug(f"ğŸ“„ ì²« ë²ˆì§¸ chunk ë‚´ìš©: {chunks[0].content[:100]}...")
            logger.debug(f"ğŸ”® ì²« ë²ˆì§¸ chunk ì„ë² ë”© ê¸¸ì´: {len(chunks[0].embedding) if chunks[0].embedding else 0}")
            
        await self.vector_store.add_chunks(chunks)
        logger.info(f"ğŸ’¾ Vector storeì— {len(chunks)}ê°œ chunk ì €ì¥ ì™„ë£Œ")
        return state

    async def retrieve_chunks(self, state: PipelineState) -> PipelineState:
        """ê´€ë ¨ ì²­í¬ ê²€ìƒ‰ ë° ë¦¬ë­í‚¹."""
        retrieved = await self.retrieval_service.retrieve(state["user_query"], k=20)
        reranked = await self.reranking_service.rerank(
            state["user_query"], retrieved, k=5
        )
        state["scratch_pad"] = ScratchPad(
            query=state["user_query"],
            chunks=reranked["chunks"],
            scores=reranked["scores"],
        )
        return state

    async def generate_answer(self, state: PipelineState) -> PipelineState:
        """ìµœì¢… ë‹µë³€ ìƒì„±."""
        context = "\n\n".join(
            [
                f"[Source: {chunk.source_url}]\n{chunk.content}"
                for chunk in state["scratch_pad"].chunks
            ]
        )

        answer = await self.llm_service.generate_answer(
            query=state["user_query"], context=context
        )

        state["response"] = QAResponse(
            query=state["user_query"],
            answer=answer,
            sources=[chunk.source_url for chunk in state["scratch_pad"].chunks],
            processing_time=datetime.now().timestamp() - state["start_time"],
        )
        return state

    async def run(self, query: str) -> QAResponse:
        """íŒŒì´í”„ë¼ì¸ ì‹¤í–‰."""
        initial_state = {
            "user_query": query,
            "start_time": datetime.now().timestamp(),
        }

        final_state = await self.graph.ainvoke(initial_state)
        return final_state["response"]
