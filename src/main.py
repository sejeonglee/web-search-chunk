"""Main entry point - Dependency Injection and Assembly."""

import asyncio
import os
import uuid
from typing import Optional
from datetime import datetime

# Core domain (ì¸í„°í˜ì´ìŠ¤ë§Œ ì‚¬ìš©)
from .core.pipeline import QAPipeline
from .core.models import IPersistentStore

# Concrete implementations (ì–´ëŒ‘í„°)
from .adapters.llm_adapter import VLLMAdapter
from .adapters.web_search_adapter import TavilySearchAdapter, GoogleSearchAdapter
from .adapters.crawling_adapter import PlaywrightCrawler
from .adapters.vector_store_adapter import FAISSVectorStore
from .adapters.persistent_store_adapter import QdrantPersistentStore
from .adapters.chunking_adapter import SimpleChunkingAdapter, ContextualChunkingAdapter
from .adapters.retrieval_adapter import HybridRetrievalAdapter
from .adapters.reranking_adapter import CrossEncoderRerankingAdapter


class DependencyContainer:
    """ì˜ì¡´ì„± ì»¨í…Œì´ë„ˆ - êµ¬ì²´ì ì¸ êµ¬í˜„ì²´ë¥¼ ìƒì„±í•˜ê³  ì£¼ì…."""

    def __init__(self, config: dict, session_id: str):
        self.config = config
        self.session_id = session_id
        self._instances = {}

    def get_llm_service(self):
        """LLM ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜."""
        if "llm_service" not in self._instances:
            self._instances["llm_service"] = VLLMAdapter(
                model_name=self.config["llm_model"],
                embedding_model=self.config.get("embedding_model", "bge-large:335m"),
                base_url=self.config.get("vllm_base_url", "http://localhost:8000/v1")
            )
        return self._instances["llm_service"]

    def get_search_service(self):
        """ê²€ìƒ‰ ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜."""
        if "search_service" not in self._instances:
            # ì„¤ì •ì— ë”°ë¼ ë‹¤ë¥¸ êµ¬í˜„ì²´ ì„ íƒ
            if self.config.get("search_provider") == "google":
                self._instances["search_service"] = GoogleSearchAdapter(
                    api_key=self.config["google_api_key"], cx=self.config["google_cx"]
                )
            else:
                self._instances["search_service"] = TavilySearchAdapter(
                    api_key=self.config["tavily_api_key"]
                )
        return self._instances["search_service"]

    def get_crawling_service(self):
        """í¬ë¡¤ë§ ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜."""
        if "crawling_service" not in self._instances:
            self._instances["crawling_service"] = PlaywrightCrawler()
        return self._instances["crawling_service"]

    def get_vector_store(self):
        """ë²¡í„° ì €ì¥ì†Œ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜."""
        if "vector_store" not in self._instances:
            self._instances["vector_store"] = FAISSVectorStore(
                dimension=self.config["vector_dimension"]
            )
        return self._instances["vector_store"]

    def get_persistent_store(self) -> IPersistentStore:
        """ì˜êµ¬ ì €ì¥ì†Œ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜."""
        if "persistent_store" not in self._instances:
            self._instances["persistent_store"] = QdrantPersistentStore(
                session_id=self.session_id,
                host=self.config.get("qdrant_host", "localhost"),
                port=self.config.get("qdrant_port", 6333)
            )
        return self._instances["persistent_store"]

    def get_chunking_service(self):
        """ì²­í‚¹ ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜."""
        if "chunking_service" not in self._instances:
            if self.config.get("chunking_strategy") == "contextual":
                self._instances["chunking_service"] = ContextualChunkingAdapter(
                    llm_service=self.get_llm_service(),
                    chunk_size=self.config["chunk_size"],
                    overlap=self.config["chunk_overlap"],
                )
            else:
                self._instances["chunking_service"] = SimpleChunkingAdapter(
                    chunk_size=self.config["chunk_size"],
                    overlap=self.config["chunk_overlap"],
                )
        return self._instances["chunking_service"]

    def get_retrieval_service(self):
        """ê²€ìƒ‰ ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜."""
        if "retrieval_service" not in self._instances:
            self._instances["retrieval_service"] = HybridRetrievalAdapter(
                vector_store=self.get_vector_store(),
                llm_service=self.get_llm_service()
            )
        return self._instances["retrieval_service"]

    def get_reranking_service(self):
        """ë¦¬ë­í‚¹ ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜."""
        if "reranking_service" not in self._instances:
            self._instances["reranking_service"] = CrossEncoderRerankingAdapter()
        return self._instances["reranking_service"]

    def build_pipeline(self) -> QAPipeline:
        """íŒŒì´í”„ë¼ì¸ ì¡°ë¦½ - ëª¨ë“  ì˜ì¡´ì„± ì£¼ì…."""
        return QAPipeline(
            llm_service=self.get_llm_service(),
            search_service=self.get_search_service(),
            crawling_service=self.get_crawling_service(),
            vector_store=self.get_vector_store(),
            chunking_service=self.get_chunking_service(),
            retrieval_service=self.get_retrieval_service(),
            reranking_service=self.get_reranking_service(),
        )


class WebSearchQASystem:
    """ì›¹ ê²€ìƒ‰ QA ì‹œìŠ¤í…œ - ìµœìƒìœ„ ì¡°ë¦½."""

    def __init__(self, config: Optional[dict] = None, session_id: Optional[str] = None):
        self.config = config or self._get_default_config()
        self.session_id = session_id or str(uuid.uuid4())
        self.container = DependencyContainer(self.config, self.session_id)
        self.pipeline = self.container.build_pipeline()
        self.persistent_store = self.container.get_persistent_store()

    def _get_default_config(self) -> dict:
        """ê¸°ë³¸ ì„¤ì •."""
        return {
            "llm_model": "Qwen/Qwen3-4B-Instruct-2507-FP8",
            "embedding_model": "bge-large:335m",
            "vllm_base_url": "http://localhost:8000/v1",
            "search_provider": "tavily",  # "tavily" or "google"
            "tavily_api_key": os.getenv("TAVILY_API_KEY", ""),
            "google_api_key": os.getenv("GOOGLE_API_KEY", ""),
            "google_cx": os.getenv("GOOGLE_CX", ""),
            "vector_dimension": 1024,  # bge-large:335mì€ 1024ì°¨ì›
            "chunk_size": 1000,
            "chunk_overlap": 200,
            "chunking_strategy": "contextual",  # "simple" or "contextual"
            "max_processing_time": 10.0,
            "qdrant_host": "localhost",
            "qdrant_port": 6333,
        }

    async def process_query(self, query: str) -> dict:
        """ì¿¼ë¦¬ ì²˜ë¦¬ (10ì´ˆ íƒ€ì„ì•„ì›ƒ)."""
        try:
            # ì´ì „ ì„¸ì…˜ ë°ì´í„° ë¡œë“œ
            await self._load_session_data()

            # íƒ€ì„ì•„ì›ƒ ì„¤ì •
            response = await asyncio.wait_for(
                self.pipeline.run(query), timeout=self.config["max_processing_time"]
            )

            # ì„¸ì…˜ ë°ì´í„° ì €ì¥
            await self._save_session_data()

            return {
                "success": True,
                "response": response.model_dump(),
                "processing_time": response.processing_time,
            }

        except asyncio.TimeoutError:
            return {
                "success": False,
                "error": "Processing exceeded 10 seconds timeout",
                "partial_response": None,
            }
        except Exception as e:
            print(f"âŒ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            import traceback
            traceback.print_exc()
            return {"success": False, "error": str(e), "partial_response": None}

    async def _load_session_data(self):
        """ì„¸ì…˜ ë°ì´í„° ë¡œë“œ."""
        try:
            chunks = await self.persistent_store.load_session()
            if chunks:
                await self.container.get_vector_store().add_chunks(chunks)
                print(f"ğŸ”„ ì„¸ì…˜ '{self.session_id}'ì—ì„œ {len(chunks)}ê°œ ê¸°ì¡´ ì²­í¬ë¥¼ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
        except:
            pass  # ì²« ì‹¤í–‰ ì‹œ ë¬´ì‹œ

    async def _save_session_data(self):
        """ì„¸ì…˜ ë°ì´í„° ì €ì¥."""
        try:
            vector_store = self.container.get_vector_store()
            # FAISSVectorStoreì˜ chunks ì†ì„± ì ‘ê·¼
            if hasattr(vector_store, "chunks"):
                print(f"ğŸ’¾ Vector storeì— ìˆëŠ” chunks ê°œìˆ˜: {len(vector_store.chunks)}")
                if vector_store.chunks:
                    await self.persistent_store.save_session(vector_store.chunks)
                    print(f"âœ… ì„¸ì…˜ '{self.session_id}'ì— {len(vector_store.chunks)}ê°œ chunkë¥¼ ì €ì¥í–ˆìŠµë‹ˆë‹¤.")
                else:
                    print("âš ï¸  Vector storeì— ì €ì¥ëœ chunksê°€ ì—†ìŠµë‹ˆë‹¤.")
            else:
                print("âŒ Vector storeì— chunks ì†ì„±ì´ ì—†ìŠµë‹ˆë‹¤.")
        except Exception as e:
            print(f"Failed to save session: {e}")
            import traceback
            traceback.print_exc()


async def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜."""
    # ì„¤ì • ë¡œë“œ (í™˜ê²½ ë³€ìˆ˜ ë˜ëŠ” ì„¤ì • íŒŒì¼ì—ì„œ)
    config = {
        "llm_model": os.getenv("LLM_MODEL", "Qwen/Qwen3-4B-Instruct-2507-FP8"),
        "embedding_model": os.getenv("EMBEDDING_MODEL", "bge-large:335m"),
        "vllm_base_url": os.getenv("VLLM_BASE_URL", "http://localhost:8000/v1"),
        "search_provider": os.getenv("SEARCH_PROVIDER", "tavily"),
        "tavily_api_key": os.getenv("TAVILY_API_KEY", ""),
        "vector_dimension": 1024,  # bge-large:335mì€ 1024ì°¨ì›
        "chunk_size": 1000,
        "chunk_overlap": 200,
        "chunking_strategy": "contextual",
        "max_processing_time": 10.0,
        "qdrant_host": os.getenv("QDRANT_HOST", "localhost"),
        "qdrant_port": int(os.getenv("QDRANT_PORT", "6333")),
    }

    # ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    system = WebSearchQASystem(config)
    print(f"ğŸ”‘ ì„¸ì…˜ ID: {system.session_id}")

    # ì˜ˆì œ ì¿¼ë¦¬ ì²˜ë¦¬
    queries = ["ìµœê·¼ AI ê¸°ìˆ  ë™í–¥ì€?", "2024ë…„ í•œêµ­ ê²½ì œ ì „ë§", "ê¸°í›„ ë³€í™” ëŒ€ì‘ ë°©ì•ˆ"]

    for query in queries:
        print(f"\nì²˜ë¦¬ ì¤‘: {query}")
        print("-" * 50)

        start_time = datetime.now()
        result = await system.process_query(query)

        if result["success"]:
            response = result["response"]
            print(f"ë‹µë³€: {response['answer'][:200]}...")
            print(f"ì†ŒìŠ¤: {', '.join(response['sources'][:3])}")
            print(f"ì²˜ë¦¬ ì‹œê°„: {result['processing_time']:.2f}ì´ˆ")
        else:
            print(f"ì˜¤ë¥˜: {result['error']}")


if __name__ == "__main__":
    # ì´ë²¤íŠ¸ ë£¨í”„ ì‹¤í–‰
    asyncio.run(main())
