from typing import List
import hashlib
from src.core.models import WebDocumentContent, SemanticChunk, IChunkingService, ILLMService
from src.utils.logger import setup_logger

logger = setup_logger(__name__)


class SimpleChunkingAdapter(IChunkingService):
    """ê°„ë‹¨í•œ ì²­í‚¹ ì–´ëŒ‘í„° - IChunkingService êµ¬í˜„."""

    def __init__(self, chunk_size: int = 1000, overlap: int = 200):
        self.chunk_size = chunk_size
        self.overlap = overlap

    async def chunk_document(
        self, document: WebDocumentContent, query: str
    ) -> List[SemanticChunk]:
        """ë¬¸ì„œë¥¼ ì˜ë¯¸ì  ì²­í¬ë¡œ ë¶„í• ."""
        chunks = []
        content = document.content

        # ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ì²­í‚¹
        for i in range(0, len(content), self.chunk_size - self.overlap):
            chunk_text = content[i : i + self.chunk_size]
            if len(chunk_text.strip()) < 50:
                continue

            chunk_id = hashlib.md5(
                f"{document.url}_{i}_{chunk_text[:50]}".encode()
            ).hexdigest()

            chunks.append(
                SemanticChunk(
                    chunk_id=chunk_id,
                    content=chunk_text,
                    source_url=document.url,
                    metadata={
                        "position": i, 
                        "query": query,
                        "parent_document_id": document.document_id,  # ë¶€ëª¨ ë¬¸ì„œ ID
                        "updated_at": document.crawl_datetime.isoformat()  # í¬ë¡¤ë§ ì‹œì 
                    },
                )
            )

        return chunks


class ContextualChunkingAdapter(IChunkingService):
    """Contextual Retrieval ê¸°ë°˜ ì²­í‚¹ ì–´ëŒ‘í„°."""

    def __init__(self, llm_service: ILLMService, chunk_size: int = 1000, overlap: int = 200):
        self.llm_service = llm_service
        self.chunk_size = chunk_size
        self.overlap = overlap

    async def chunk_document(
        self, document: WebDocumentContent, query: str
    ) -> List[SemanticChunk]:
        """LLMì„ ì‚¬ìš©í•œ ì»¨í…ìŠ¤íŠ¸ ì¶”ê°€ ì²­í‚¹."""
        logger.info(f"ğŸ” Contextual Retrieval ì²­í‚¹ ì‹œì‘: {document.url}")
        
        # 1ë‹¨ê³„: ê¸°ë³¸ ì²­í‚¹
        raw_chunks = self._create_raw_chunks(document.content, document.url)
        logger.debug(f"  ê¸°ë³¸ ì²­í‚¹ ì™„ë£Œ: {len(raw_chunks)}ê°œ")
        
        if not raw_chunks:
            return []
            
        # 2ë‹¨ê³„: ê° ì²­í¬ì— ì»¨í…ìŠ¤íŠ¸ ì¶”ê°€ (LLM ì‚¬ìš©)
        contextual_chunks = await self._add_context_to_chunks(
            raw_chunks, document.content, query, document
        )
        
        logger.info(f"âœ… Contextual ì²­í‚¹ ì™„ë£Œ: {len(contextual_chunks)}ê°œ")
        return contextual_chunks
    
    def _create_raw_chunks(self, content: str, url: str) -> List[dict]:
        """ê¸°ë³¸ ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ì²­í‚¹."""
        chunks = []
        
        for i in range(0, len(content), self.chunk_size - self.overlap):
            chunk_text = content[i : i + self.chunk_size]
            if len(chunk_text.strip()) < 50:
                continue
                
            chunks.append({
                'text': chunk_text,
                'position': i,
                'url': url
            })
                
        return chunks
    
    async def _add_context_to_chunks(self, raw_chunks: List[dict], full_document: str, query: str, document: WebDocumentContent) -> List[SemanticChunk]:
        """ê° ì²­í¬ì— ì»¨í…ìŠ¤íŠ¸ ì¶”ê°€ (VLLM batch inference ì‚¬ìš©)."""
        logger.debug(f"ğŸ¤– {len(raw_chunks)}ê°œ ì²­í¬ì— VLLM ë°°ì¹˜ë¡œ ì»¨í…ìŠ¤íŠ¸ ì¶”ê°€ ì¤‘...")
        
        # ëª¨ë“  ì²­í¬ì— ëŒ€í•œ í”„ë¡¬í”„íŠ¸ ìƒì„±
        prompts = []
        for i, chunk_data in enumerate(raw_chunks):
            prompt = self._create_context_prompt(chunk_data['text'], full_document)
            prompts.append(prompt)
        
        try:
            # ê°œë³„ LLM í˜¸ì¶œì„ ë³‘ë ¬ë¡œ ì²˜ë¦¬
            import asyncio
            logger.info(f"ğŸ“¦ LLM ê°œë³„ ì¶”ë¡  ì‹œì‘: {len(prompts)}ê°œ í”„ë¡¬í”„íŠ¸")
            tasks = [self.llm_service.generate_answer("", prompt) for prompt in prompts]
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            contextual_responses = []
            for result in results:
                if isinstance(result, Exception):
                    logger.warning(f"ê°œë³„ LLM í˜¸ì¶œ ì‹¤íŒ¨: {str(result)}")
                    contextual_responses.append("")
                else:
                    contextual_responses.append(result)
                
            logger.info(f"âœ… LLM ê°œë³„ ì¶”ë¡  ì™„ë£Œ: {len(contextual_responses)}ê°œ ì‘ë‹µ")
            
        except Exception as e:
            logger.error(f"âŒ LLM ì¶”ë¡  ì‹¤íŒ¨: {str(e)}")
            # í´ë°±: ì›ë³¸ ì²­í¬ë“¤ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©
            contextual_responses = [chunk['text'] for chunk in raw_chunks]
        
        # SemanticChunk ê°ì²´ ìƒì„±
        semantic_chunks = []
        for i, (chunk_data, context_response) in enumerate(zip(raw_chunks, contextual_responses)):
            # ì»¨í…ìŠ¤íŠ¸ + ì›ë³¸ ì²­í¬ ê²°í•©
            if context_response and context_response.strip():
                contextual_content = f"{context_response.strip()}\\n\\n{chunk_data['text']}"
            else:
                logger.warning(f"  ì²­í¬ {i+1} ì»¨í…ìŠ¤íŠ¸ ìƒì„± ì‹¤íŒ¨, ì›ë³¸ ì‚¬ìš©")
                contextual_content = chunk_data['text']
            
            chunk_id = hashlib.md5(
                f"{chunk_data['url']}_{chunk_data['position']}_{chunk_data['text'][:50]}".encode()
            ).hexdigest()
            
            semantic_chunks.append(
                SemanticChunk(
                    chunk_id=chunk_id,
                    content=contextual_content,  # ì»¨í…ìŠ¤íŠ¸ê°€ ì¶”ê°€ëœ ì»¨í…ì¸ 
                    source_url=chunk_data['url'],
                    metadata={
                        'position': chunk_data['position'],
                        'query': query,
                        'original_content': chunk_data['text'],  # ì›ë³¸ ì»¨í…ì¸ ë„ ì €ì¥
                        'contextual_retrieval': True,
                        'parent_document_id': document.document_id,  # ë¶€ëª¨ ë¬¸ì„œ ID
                        'updated_at': document.crawl_datetime.isoformat()  # í¬ë¡¤ë§ ì‹œì 
                    }
                )
            )
        
        return semantic_chunks
    
    def _create_context_prompt(self, chunk_text: str, full_document: str) -> str:
        """ì²­í¬ì— ëŒ€í•œ ì»¨í…ìŠ¤íŠ¸ ìƒì„± í”„ë¡¬í”„íŠ¸ ì‘ì„±."""
        return f"""
<document>
{full_document}
</document>

Here is the chunk we want to situate within the whole document:
<chunk>
{chunk_text}
</chunk>

Please give a short succinct context to situate this chunk within the overall document for the purpose of improving search retrieval. The context should be in Korean and help identify what this chunk is about in relation to the full document.

ë‹¤ìŒ ì‚¬í•­ì„ ê³ ë ¤í•˜ì—¬ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”:
1. ì´ ì²­í¬ê°€ ì „ì²´ ë¬¸ì„œì—ì„œ ì–´ë–¤ ì£¼ì œë‚˜ ì„¹ì…˜ì— ì†í•˜ëŠ”ì§€
2. ì „ì²´ ë¬¸ë§¥ì—ì„œ ì´ ì •ë³´ì˜ ì—­í• ì´ ë¬´ì—‡ì¸ì§€
3. ê²€ìƒ‰ ì‹œ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ëŠ” ë° ë„ì›€ì´ ë˜ëŠ” í‚¤ì›Œë“œ

ì»¨í…ìŠ¤íŠ¸ëŠ” 1-2ë¬¸ì¥ ë‚´ì—ì„œ ê°„ê²°í•˜ê²Œ ì‘ì„±í•´ì£¼ì„¸ìš”.
"""
    
    async def _generate_context_for_chunk(self, chunk_text: str, full_document: str, query: str, chunk_num: int, total_chunks: int) -> str:
        """ë‹¨ì¼ ì²­í¬ì— ëŒ€í•œ ì»¨í…ìŠ¤íŠ¸ ìƒì„±."""
        # Anthropic Contextual Retrieval í”„ë¡¬í”„íŠ¸ ê¸°ë°˜
        prompt = f"""
<document>
{full_document}
</document>

Here is the chunk we want to situate within the whole document:
<chunk>
{chunk_text}
</chunk>

Please give a short succinct context to situate this chunk within the overall document for the purpose of improving search retrieval. The context should be in Korean and help identify what this chunk is about in relation to the full document.

ë‹¤ìŒ ì‚¬í•­ì„ ê³ ë ¤í•˜ì—¬ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”:
1. ì´ ì²­í¬ê°€ ì „ì²´ ë¬¸ì„œì—ì„œ ì–´ë–¤ ì£¼ì œë‚˜ ì„¹ì…˜ì— ì†í•˜ëŠ”ì§€
2. ì „ì²´ ë¬¸ë§¥ì—ì„œ ì´ ì •ë³´ì˜ ì—­í• ì´ ë¬´ì—‡ì¸ì§€
3. ê²€ìƒ‰ ì‹œ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ëŠ” ë° ë„ì›€ì´ ë˜ëŠ” í‚¤ì›Œë“œ

ì»¨í…ìŠ¤íŠ¸ëŠ” 1-2ë¬¸ì¥ ë‚´ì—ì„œ ê°„ê²°í•˜ê²Œ ì‘ì„±í•´ì£¼ì„¸ìš”.
"""
        
        try:
            # LLMì„ ì‚¬ìš©í•˜ì—¬ ì»¨í…ìŠ¤íŠ¸ ìƒì„±
            context = await self.llm_service.generate_answer(query="", context=prompt)
            
            # ì»¨í…ìŠ¤íŠ¸ + ì›ë³¸ ì²­í¬ ê²°í•©
            contextual_chunk = f"{context.strip()}\n\n{chunk_text}"
            
            logger.debug(f"    ì²­í¬ {chunk_num}/{total_chunks} ì»¨í…ìŠ¤íŠ¸ ìƒì„± ì™„ë£Œ")
            return contextual_chunk
            
        except Exception as e:
            logger.error(f"    ì²­í¬ {chunk_num}/{total_chunks} ì»¨í…ìŠ¤íŠ¸ ìƒì„± ì‹¤íŒ¨: {str(e)}")
            # ì‹¤íŒ¨ ì‹œ ì›ë³¸ ì²­í¬ ë°˜í™˜
            return chunk_text
