from typing import List, Dict, Any
import re
import math
from collections import Counter
from src.core.models import SemanticChunk, IRetrievalService, IVectorStore


class HybridRetrievalAdapter(IRetrievalService):
    """í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì–´ëŒ‘í„° - IRetrievalService êµ¬í˜„."""

    def __init__(self, vector_store: IVectorStore, llm_service=None):
        self.vector_store = vector_store
        self.llm_service = llm_service  # LLM ì„œë¹„ìŠ¤ (ì„ë² ë”© ìƒì„±ìš©)

    async def retrieve(self, query: str, k: int = 20) -> List[SemanticChunk]:
        """í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ìˆ˜í–‰."""
        # 1. ì„ë² ë”© ìƒì„±
        if self.llm_service:
            try:
                embeddings = await self.llm_service.get_embeddings([query])
                query_embedding = embeddings[0]
                print(f"ğŸ”® ì¿¼ë¦¬ ì„ë² ë”© ìƒì„± ì™„ë£Œ: {len(query_embedding)}ì°¨ì›")
            except Exception as e:
                print(f"âŒ ì¿¼ë¦¬ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {str(e)}")
                query_embedding = [0.1] * 1024  # ë”ë¯¸ ì„ë² ë”© (1024ì°¨ì›)
        else:
            query_embedding = [0.1] * 1024  # ë”ë¯¸ ì„ë² ë”© (1024ì°¨ì›)

        # 2. ë²¡í„° ê²€ìƒ‰
        vector_results = await self.vector_store.search(query_embedding, k)

        # 3. BM25 ê²€ìƒ‰ (ê°„ë‹¨í•œ êµ¬í˜„)
        keyword_results = await self._bm25_search(query, k)

        # 4. RRF ìœµí•©
        combined = self._reciprocal_rank_fusion(vector_results, keyword_results)

        return [item["chunk"] for item in combined[:k]]

    async def _bm25_search(self, query: str, k: int) -> List[Dict[str, Any]]:
        """BM25 ê¸°ë°˜ ê²€ìƒ‰."""
        # Vector storeì—ì„œ ëª¨ë“  chunks ê°€ì ¸ì˜¤ê¸°
        if not hasattr(self.vector_store, 'chunks'):
            return []
        
        chunks = getattr(self.vector_store, 'chunks', [])
        
        # ì¿¼ë¦¬ í† í°í™”
        query_tokens = self._tokenize(query.lower())
        if not query_tokens:
            return []
        
        # ë¬¸ì„œë³„ í† í°í™” ë° í†µê³„ ê³„ì‚°
        documents = []
        doc_lengths = []
        
        for chunk in chunks:
            tokens = self._tokenize(chunk.content.lower())
            documents.append(tokens)
            doc_lengths.append(len(tokens))
        
        if not documents:
            return []
        
        # í‰ê·  ë¬¸ì„œ ê¸¸ì´
        avgdl = sum(doc_lengths) / len(doc_lengths)
        
        # ê° ì¿¼ë¦¬ í† í°ì´ ë“±ì¥í•˜ëŠ” ë¬¸ì„œ ìˆ˜ ê³„ì‚° (IDF ê³„ì‚°ìš©)
        df = {}  # document frequency
        for tokens in documents:
            unique_tokens = set(tokens)
            for token in query_tokens:
                if token in unique_tokens:
                    df[token] = df.get(token, 0) + 1
        
        # BM25 íŒŒë¼ë¯¸í„°
        k1 = 1.2
        b = 0.75
        N = len(documents)
        
        scores = []
        
        for i, tokens in enumerate(documents):
            score = 0.0
            token_counts = Counter(tokens)
            doc_len = doc_lengths[i]
            
            for token in query_tokens:
                if token in token_counts:
                    tf = token_counts[token]  # term frequency
                    idf = math.log((N - df.get(token, 0) + 0.5) / (df.get(token, 0) + 0.5))
                    
                    # BM25 ê³µì‹
                    numerator = idf * tf * (k1 + 1)
                    denominator = tf + k1 * (1 - b + b * (doc_len / avgdl))
                    score += numerator / denominator
            
            scores.append((i, score))
        
        # ì ìˆ˜ìˆœìœ¼ë¡œ ì •ë ¬
        scores.sort(key=lambda x: x[1], reverse=True)
        
        # ìƒìœ„ kê°œ ê²°ê³¼ ë°˜í™˜
        results = []
        for i, score in scores[:k]:
            if score > 0:  # ì ìˆ˜ê°€ 0ë³´ë‹¤ í° ê²½ìš°ë§Œ
                results.append({
                    "chunk": chunks[i],
                    "score": score
                })
        
        return results
    
    def _tokenize(self, text: str) -> List[str]:
        """í…ìŠ¤íŠ¸ í† í°í™” (ê°„ë‹¨í•œ êµ¬í˜„)."""
        # ì•ŒíŒŒë²³, ìˆ«ì, í•œê¸€ë§Œ ì¶”ì¶œí•˜ê³  ì†Œë¬¸ìë¡œ ë³€í™˜
        tokens = re.findall(r'[a-zA-Z0-9ê°€-í£]+', text)
        # 2ê¸€ì ì´ìƒì¸ í† í°ë§Œ ë°˜í™˜
        return [token for token in tokens if len(token) >= 2]

    def _reciprocal_rank_fusion(
        self, vector_results: List[Dict], keyword_results: List[Dict], k: int = 60
    ) -> List[Dict]:
        """RRFë¥¼ ì‚¬ìš©í•œ ê²°ê³¼ ìœµí•©."""
        scores = {}

        for rank, item in enumerate(vector_results):
            chunk_id = item["chunk"].chunk_id
            scores[chunk_id] = scores.get(chunk_id, 0) + 1 / (k + rank + 1)

        for rank, item in enumerate(keyword_results):
            chunk_id = item["chunk"].chunk_id
            scores[chunk_id] = scores.get(chunk_id, 0) + 1 / (k + rank + 1)

        # ê²°ê³¼ ì •ë ¬ ë° ë°˜í™˜
        sorted_chunks = sorted(scores.items(), key=lambda x: x[1], reverse=True)

        chunk_map = {
            item["chunk"].chunk_id: item for item in vector_results + keyword_results
        }

        result = []
        for chunk_id, score in sorted_chunks:
            if chunk_id in chunk_map:
                result.append(chunk_map[chunk_id])

        return result
