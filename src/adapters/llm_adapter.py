"""Adapter implementations for external services - implements core interfaces."""

# src/adapters/llm_adapter.py
from typing import List
import httpx
import json
from src.core.models import SearchQuery, ILLMService
from src.utils.logger import setup_logger

logger = setup_logger(__name__)


class VLLMAdapter(ILLMService):
    """VLLM ì„œë¹™ LLM ì–´ëŒ‘í„° - ILLMService êµ¬í˜„."""

    def __init__(
        self,
        model_name: str = "Qwen/Qwen3-4B-Instruct-2507-FP8",
        embedding_model: str = "bge-large:335m",
        base_url: str = "http://localhost:8000/v1",
        embedding_base_url: str = "http://localhost:11434/v1",
    ):
        self.model_name = model_name
        self.embedding_model = embedding_model
        self.base_url = base_url
        self.embedding_base_url = embedding_base_url
        self.client = httpx.AsyncClient(timeout=30.0)

    async def generate_queries(self, user_query: str) -> List[SearchQuery]:
        """ë©€í‹° ì¿¼ë¦¬ ìƒì„± êµ¬í˜„."""
        logger.debug(f"ğŸ¤– LLM ì¿¼ë¦¬ ìƒì„± ì‹œì‘: {user_query}")
        prompt = f"""ë‹¤ìŒ ì‚¬ìš©ì ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ ì›¹ ê²€ìƒ‰ì— ì í•©í•œ 3ê°œì˜ ë‹¤ì–‘í•œ ê²€ìƒ‰ ì¿¼ë¦¬ë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”.
ì›ë³¸ ì§ˆë¬¸: {user_query}

ê° ì¿¼ë¦¬ëŠ” ì„œë¡œ ë‹¤ë¥¸ ê´€ì ì´ë‚˜ í‚¤ì›Œë“œë¥¼ ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤.

ì‘ë‹µ í˜•ì‹:
1. [ì²« ë²ˆì§¸ ê²€ìƒ‰ ì¿¼ë¦¬]
2. [ë‘ ë²ˆì§¸ ê²€ìƒ‰ ì¿¼ë¦¬]  
3. [ì„¸ ë²ˆì§¸ ê²€ìƒ‰ ì¿¼ë¦¬]"""

        try:
            response = await self._call_vllm(prompt)
            # ì‘ë‹µì—ì„œ ì¿¼ë¦¬ ì¶”ì¶œ
            lines = response.strip().split("\n")
            processed_queries = []
            for line in lines:
                if line.strip() and any(line.startswith(f"{i}.") for i in range(1, 10)):
                    query = line.split(".", 1)[1].strip()
                    if query:
                        processed_queries.append(query)

            # ìµœì†Œ 1ê°œ ì¿¼ë¦¬ëŠ” ë³´ì¥
            if not processed_queries:
                processed_queries = [user_query]

            logger.info(f"âœ… {len(processed_queries)}ê°œ ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„± ì™„ë£Œ")
            return [
                SearchQuery(
                    original_query=user_query,
                    processed_queries=processed_queries[:3],  # ìµœëŒ€ 3ê°œ
                )
            ]

        except Exception as e:
            logger.error(f"âŒ ì¿¼ë¦¬ ìƒì„± ì‹¤íŒ¨: {str(e)}")
            # ì—ëŸ¬ ì‹œ ê¸°ë³¸ ì¿¼ë¦¬ ë°˜í™˜
            return [
                SearchQuery(original_query=user_query, processed_queries=[user_query])
            ]

    async def generate_answer(self, query: str, context: str) -> str:
        """ë‹µë³€ ìƒì„± êµ¬í˜„."""

        prompt = f"""ë‹¤ìŒ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ì ì§ˆë¬¸ì— ëŒ€í•´ ì •í™•í•˜ê³  ìœ ìš©í•œ ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”.

ì»¨í…ìŠ¤íŠ¸:
{context}

ì§ˆë¬¸: {query}

ë‹µë³€ì€ ë‹¤ìŒ ì¡°ê±´ì„ ë§Œì¡±í•´ì•¼ í•©ë‹ˆë‹¤:
1. ì»¨í…ìŠ¤íŠ¸ì— ê¸°ë°˜í•œ ì •í™•í•œ ì •ë³´ ì œê³µ
2. ëª…í™•í•˜ê³  ì´í•´í•˜ê¸° ì‰¬ìš´ í•œêµ­ì–´
3. ê°€ëŠ¥í•œ í•œ êµ¬ì²´ì ì¸ ì •ë³´ í¬í•¨
4. ì¶œì²˜ê°€ ë¶ˆë¶„ëª…í•œ ì •ë³´ëŠ” í¬í•¨í•˜ì§€ ì•ŠìŒ

ë‹µë³€:"""

        try:
            logger.debug(f"ğŸ’­ ë‹µë³€ ìƒì„± ì‹œì‘: {prompt}")
            answer = await self._call_vllm(prompt)
            logger.info(f"âœ… ë‹µë³€ ìƒì„± ì™„ë£Œ: {len(answer)}ì")
            return answer
        except Exception as e:
            logger.error(f"âŒ ë‹µë³€ ìƒì„± ì‹¤íŒ¨: {str(e)}")
            return f"ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"

    async def _call_vllm(self, prompt: str) -> str:
        """VLLM ì„œë¹™ API í˜¸ì¶œ."""
        try:
            payload = {
                "model": self.model_name,
                "messages": [{"role": "user", "content": prompt}],
                "stream": False,
                "temperature": 0.1,
                "max_tokens": 1024,
            }

            logger.debug(f"ğŸ“¡ LLM API í˜¸ì¶œ: {self.base_url}")
            response = await self.client.post(
                f"{self.base_url}/chat/completions",
                json=payload,
                headers={"Content-Type": "application/json"},
            )
            response.raise_for_status()

            result = response.json()
            logger.debug(
                f"ğŸ“¥ LLM ì‘ë‹µ ìˆ˜ì‹ : {len(result['choices'][0]['message']['content'])}ì"
            )
            return result["choices"][0]["message"]["content"]

        except Exception as e:
            logger.error(f"âŒ VLLM API í˜¸ì¶œ ì‹¤íŒ¨: {str(e)}")
            raise Exception(f"VLLM API í˜¸ì¶œ ì‹¤íŒ¨: {str(e)}")

    async def get_embeddings(self, texts: List[str]) -> List[List[float]]:
        """í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„± (VLLM ì„ë² ë”© ì„œë¹™)."""
        logger.debug(f"ğŸ”® ì„ë² ë”© ìƒì„± ì‹œì‘: {len(texts)}ê°œ í…ìŠ¤íŠ¸")
        try:
            embeddings = []
            for i, text in enumerate(texts):
                payload = {"model": self.embedding_model, "input": text}

                response = await self.client.post(
                    f"{self.embedding_base_url}/embeddings",
                    json=payload,
                    headers={"Content-Type": "application/json"},
                )
                response.raise_for_status()

                result = response.json()
                embeddings.append(result["data"][0]["embedding"])
                logger.debug(f"  í…ìŠ¤íŠ¸ {i + 1}/{len(texts)} ì„ë² ë”© ì™„ë£Œ")

            logger.info(f"âœ… ì´ {len(embeddings)}ê°œ ì„ë² ë”© ìƒì„± ì™„ë£Œ")
            return embeddings

        except Exception as e:
            logger.error(f"âŒ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {str(e)}")
            raise Exception(f"ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {str(e)}")

    async def __aenter__(self):
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        await self.client.aclose()
